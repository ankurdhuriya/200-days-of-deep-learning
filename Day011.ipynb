{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2678bc26"
      },
      "source": [
        "# Tokenizers\n",
        "\n",
        "Tokenizers are fundamental components in Natural Language Processing (NLP) that convert raw text into a sequence of smaller units called tokens. These tokens can be words, subwords, or even characters. Tokenization is a crucial first step before feeding text data into NLP models. Different tokenizers employ various strategies to balance vocabulary size, handling of out-of-vocabulary (OOV) words, and computational efficiency.\n",
        "\n",
        "Here's an overview of three common subword tokenizers: BPE, WordPiece, and SentencePiece.\n",
        "\n",
        "### Byte-Pair Encoding (BPE)\n",
        "\n",
        "**Working Principle:**\n",
        "BPE is a greedy algorithm that iteratively merges the most frequent pairs of bytes (or characters) into new, larger tokens. It starts with a base vocabulary of individual characters and repeatedly adds the most frequent byte pairs to the vocabulary until a desired vocabulary size is reached or no more frequent pairs exist.\n",
        "\n",
        "**Merge Principle:**\n",
        "The core idea is to identify the most frequent adjacent pair of characters or subwords in the training corpus and replace all occurrences of this pair with a new, combined token. This process is repeated, creating a vocabulary of subword units of varying lengths.\n",
        "\n",
        "**OOV Handling:**\n",
        "BPE handles OOV words by breaking them down into smaller, known subword units or individual characters. Since the base vocabulary includes all individual characters, any word can ultimately be represented as a sequence of characters if no larger subword units are found.\n",
        "\n",
        "**Advantages:**\n",
        "- Relatively simple to implement.\n",
        "- Effectively reduces vocabulary size while handling OOV words.\n",
        "- Generates meaningful subword units.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Can create very long tokens if frequent pairs are long sequences.\n",
        "- The greedy approach might not always result in the optimal tokenization.\n",
        "\n",
        "### WordPiece\n",
        "\n",
        "**Working Principle:**\n",
        "WordPiece is similar to BPE but differs in its merging strategy. Instead of merging the most frequent pairs, WordPiece merges pairs that maximize the likelihood of the training data when added to the vocabulary. It starts with a vocabulary of individual characters and iteratively adds the pair of units that, when merged, results in the greatest increase in the product of their probabilities (or likelihood).\n",
        "\n",
        "**Merge Principle:**\n",
        "WordPiece considers the probability of a pair of tokens appearing together. It merges the pair `(A, B)` if the probability of `AB` is higher than the product of the probabilities of `A` and `B` individually, normalized by the frequency of `AB`. This can be seen as merging pairs that are statistically more likely to appear together.\n",
        "\n",
        "**OOV Handling:**\n",
        "Similar to BPE, WordPiece handles OOV words by breaking them down into smaller, known subword units. It typically uses a special prefix (e.g., `##` in BERT) to indicate that a subword is not the beginning of a word.\n",
        "\n",
        "**Advantages:**\n",
        "- Often produces a more linguistically motivated tokenization than BPE.\n",
        "- Used in popular models like BERT.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Can be slightly more complex to implement than BPE.\n",
        "\n",
        "### SentencePiece\n",
        "\n",
        "**Working Principle:**\n",
        "SentencePiece is unique in that it treats the input as a raw stream of characters and directly learns a vocabulary of subword units. It does not rely on pre-splitting the text into words using whitespace. This makes it suitable for languages without explicit word boundaries (e.g., Chinese, Japanese). SentencePiece can implement both BPE and unigram language model based tokenization.\n",
        "\n",
        "**Merge Principle (for BPE mode):**\n",
        "Similar to BPE, it iteratively merges the most frequent character or subword sequences.\n",
        "\n",
        "**Merge Principle (for Unigram Language Model mode):**\n",
        "It learns a probability distribution over the vocabulary and tokenizes the text by finding the most likely sequence of subword units that reconstructs the original text.\n",
        "\n",
        "**OOV Handling:**\n",
        "SentencePiece can handle OOV words by breaking them down into smaller subword units or individual characters, similar to BPE and WordPiece. Its ability to handle raw character streams makes it robust to variations in whitespace and punctuation.\n",
        "\n",
        "**Advantages:**\n",
        "- Handles languages without explicit word boundaries effectively.\n",
        "- Can produce reversible tokenization (decode tokens back to the original text).\n",
        "- Supports both BPE and unigram language model approaches.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Can be more computationally intensive during training compared to basic BPE.\n",
        "\n",
        "In summary, BPE, WordPiece, and SentencePiece are powerful subword tokenization techniques that offer different approaches to balancing vocabulary size and OOV handling. The choice of tokenizer often depends on the specific language, dataset, and the requirements of the NLP task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YI-HsAH7vlUR"
      },
      "outputs": [],
      "source": [
        "from tokenizers.models import BPE, WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
        "from tokenizers import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BPE Encoding"
      ],
      "metadata": {
        "id": "3tGPMqeHvvJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer = Tokenizer(BPE())"
      ],
      "metadata": {
        "id": "MNPiA9AewOBa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRRqE7Wjwjew",
        "outputId": "269d1522-85e0-4935-b27d-50457753505f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=None, pre_tokenizer=None, post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={}, merges=[]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "gb-LfQpIwkNW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a1M5d72wsMl",
        "outputId": "8810ec2a-d155-4dd7-b8cd-d41c2d42fdc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={}, merges=[]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(vocab_size=1000, min_frequency=2, special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"])\n",
        "trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYbmTAm3wn56",
        "outputId": "f9930f2d-5d70-4cba-ac57-0085636d3f6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BpeTrainer(BpeTrainer(min_frequency=2, vocab_size=1000, show_progress=True, special_tokens=[AddedToken(content=\"<unk>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"<pad>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"<s>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"</s>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True)], limit_alphabet=None, initial_alphabet=[], continuing_subword_prefix=None, end_of_word_suffix=None, max_token_length=None, words={}))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.train(files=['corpus.txt'], trainer=trainer)"
      ],
      "metadata": {
        "id": "QgZcO-V8wyrH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncX0ppyBxTlx",
        "outputId": "1b0d1cc4-f06d-4695-caca-4c94e3dc035d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"<unk>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"<pad>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"<s>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"</s>\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"<unk>\":0, \"<pad>\":1, \"<s>\":2, \"</s>\":3, \",\":4, \".\":5, \":\":6, \"A\":7, \"C\":8, \"E\":9, \"I\":10, \"J\":11, \"L\":12, \"P\":13, \"S\":14, \"a\":15, \"b\":16, \"c\":17, \"d\":18, \"e\":19, \"f\":20, \"g\":21, \"h\":22, \"i\":23, \"k\":24, \"l\":25, \"m\":26, \"n\":27, \"o\":28, \"p\":29, \"q\":30, \"r\":31, \"s\":32, \"t\":33, \"u\":34, \"v\":35, \"w\":36, \"x\":37, \"y\":38, \"z\":39, \"—\":40, \"ti\":41, \"an\":42, \"in\":43, \"es\":44, \"on\":45, \"er\":46, \"ati\":47, \"or\":48, \"and\":49, \"le\":50, \"ers\":51, \"al\":52, \"ic\":53, \"it\":54, \"te\":55, \"ation\":56, \"ap\":57, \"ng\":58, \"ol\":59, \"op\":60, \"ri\":61, \"ab\":62, \"ac\":63, \"ar\":64, \"at\":65, \"de\":66, \"en\":67, \"ms\":68, \"ing\":69, \"ce\":70, \"ch\":71, \"ge\":72, \"li\":73, \"lo\":74, \"ne\":75, \"ot\":76, \"pl\":77, \"re\":78, \"un\":79, \"ication\":80, \"ith\":81, \"ence\":82, \"ad\":83, \"as\":84, \"age\":85, \"br\":86, \"bu\":87, \"co\":88, \"con\":89, \"cre\":90, \"dat\":91, \"el\":92, \"em\":93, \"hon\":94, \"is\":95, \"iti\":96, \"mar\":97, \"mat\":98, ...}, merges=[(\"t\", \"i\"), (\"a\", \"n\"), (\"i\", \"n\"), (\"e\", \"s\"), (\"o\", \"n\"), (\"e\", \"r\"), (\"a\", \"ti\"), (\"o\", \"r\"), (\"an\", \"d\"), (\"l\", \"e\"), (\"er\", \"s\"), (\"a\", \"l\"), (\"i\", \"c\"), (\"i\", \"t\"), (\"t\", \"e\"), (\"ati\", \"on\"), (\"a\", \"p\"), (\"n\", \"g\"), (\"o\", \"l\"), (\"o\", \"p\"), (\"r\", \"i\"), (\"a\", \"b\"), (\"a\", \"c\"), (\"a\", \"r\"), (\"a\", \"t\"), (\"d\", \"e\"), (\"e\", \"n\"), (\"m\", \"s\"), (\"in\", \"g\"), (\"c\", \"e\"), (\"c\", \"h\"), (\"g\", \"e\"), (\"l\", \"i\"), (\"l\", \"o\"), (\"n\", \"e\"), (\"o\", \"t\"), (\"p\", \"l\"), (\"r\", \"e\"), (\"u\", \"n\"), (\"ic\", \"ation\"), (\"it\", \"h\"), (\"en\", \"ce\"), (\"a\", \"d\"), (\"a\", \"s\"), (\"a\", \"ge\"), (\"b\", \"r\"), (\"b\", \"u\"), (\"c\", \"o\"), (\"c\", \"on\"), (\"c\", \"re\"), (\"d\", \"at\"), (\"e\", \"l\"), (\"e\", \"m\"), (\"h\", \"on\"), (\"i\", \"s\"), (\"i\", \"ti\"), (\"m\", \"ar\"), (\"m\", \"at\"), (\"n\", \"ing\"), (\"o\", \"s\"), (\"o\", \"u\"), (\"p\", \"r\"), (\"p\", \"or\"), (\"q\", \"u\"), (\"s\", \"ol\"), (\"t\", \"o\"), (\"t\", \"r\"), (\"t\", \"s\"), (\"t\", \"ers\"), (\"u\", \"ti\"), (\"v\", \"es\"), (\"w\", \"or\"), (\"w\", \"ith\"), (\"ti\", \"m\"), (\"in\", \"te\"), (\"es\", \"s\"), (\"or\", \"m\"), (\"it\", \"y\"), (\"op\", \"tim\"), (\"ab\", \"le\"), (\"ac\", \"k\"), (\"ar\", \"ning\"), (\"pl\", \"ication\"), (\"ad\", \"ap\"), (\"cre\", \"ati\"), (\"mar\", \"t\"), (\"por\", \"t\"), (\"plication\", \"s\")]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BPE Tokenizer Configuration\n",
        "\n",
        "- **Special Tokens:**  \n",
        "  - `<unk>`, `<pad>`, `<s>`, `</s>`  \n",
        "    These tokens are added to handle unknown words, padding, sequence starts, and ends.\n",
        "\n",
        "- **Pre-Tokenizer:**  \n",
        "  - `Whitespace`  \n",
        "    Text is split using whitespace before tokenization.\n",
        "\n",
        "- **Model:**  \n",
        "  - `BPE`  \n",
        "    The Byte-Pair Encoding model uses its learned vocabulary and merge rules.\n",
        "\n",
        "- **Vocabulary:**  \n",
        "  - Contains the learned tokens:  \n",
        "    - Characters  \n",
        "    - Subwords  \n",
        "    - Words  \n",
        "  - Each token is mapped to a unique ID.\n",
        "\n",
        "- **Merges:**  \n",
        "  - Lists pairs of tokens that were merged during training.  \n",
        "  - These merges form larger subword units from smaller ones."
      ],
      "metadata": {
        "id": "bPW2IR1SyvKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bpe_tokenizer.save(\"bpe-tokenizer.json\")\n",
        "loaded_bpe = Tokenizer.from_file(\"bpe-tokenizer.json\")"
      ],
      "metadata": {
        "id": "Q8Alf4xjxZ7g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = loaded_bpe.encode(\"huggingface and transformers\")\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3TpyKIFxiwj",
        "outputId": "ee5efa80-0283-48a4-c3bc-6136251dea1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['h', 'u', 'g', 'g', 'ing', 'f', 'ac', 'e', 'and', 'tr', 'an', 's', 'f', 'orm', 'ers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows the tokens generated by the **BPE tokenizer** for the input string:\n",
        "\n",
        "- The tokenizer breaks down the words into subword units based on the learned vocabulary and merges.\n",
        "\n",
        "- Example token breakdown:  \n",
        "  - `\"huggingface\"` is split into:  \n",
        "    `'h'`, `'u'`, `'g'`, `'g'`, `'ing'`, `'f'`, `'ac'`, `'e'`  \n",
        "  - `\"transformers\"` is split into:  \n",
        "    `'tr'`, `'an'`, `'s'`, `'f'`, `'orm'`, `'ers'`\n",
        "\n",
        "- The word `\"and\"` is kept as a **single token** because it likely exists as a whole word in the vocabulary.\n",
        "\n",
        "This demonstrates how BPE efficiently represents words by splitting complex words into frequent subword units while retaining common words intact."
      ],
      "metadata": {
        "id": "Ur4h6LGTzE0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordPiece Tokenizer"
      ],
      "metadata": {
        "id": "I_HedMeuzcik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = WordPieceTrainer(vocab_size=1000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ],
      "metadata": {
        "id": "hVR-chbjxlw-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpiece_tokenizer.train(['corpus.txt'], trainer)\n",
        "wordpiece_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPS6-v_czwY4",
        "outputId": "bf40fb3c-868a-4122-8a30-7e139af74823"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"[CLS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"[SEP]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":4, \"content\":\"[MASK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100, vocab={\"[UNK]\":0, \"[CLS]\":1, \"[SEP]\":2, \"[PAD]\":3, \"[MASK]\":4, \",\":5, \".\":6, \":\":7, \"A\":8, \"C\":9, \"E\":10, \"I\":11, \"J\":12, \"L\":13, \"P\":14, \"S\":15, \"a\":16, \"b\":17, \"c\":18, \"d\":19, \"e\":20, \"f\":21, \"g\":22, \"h\":23, \"i\":24, \"k\":25, \"l\":26, \"m\":27, \"n\":28, \"o\":29, \"p\":30, \"q\":31, \"r\":32, \"s\":33, \"t\":34, \"u\":35, \"v\":36, \"w\":37, \"x\":38, \"y\":39, \"z\":40, \"—\":41, \"##u\":42, \"##c\":43, \"##e\":44, \"##s\":45, \"##o\":46, \"##l\":47, \"##v\":48, \"##i\":49, \"##f\":50, \"##n\":51, \"##g\":52, \"##r\":53, \"##d\":54, \"##w\":55, \"##h\":56, \"##k\":57, \"##b\":58, \"##m\":59, \"##t\":60, \"##a\":61, \"##y\":62, \"##p\":63, \"##z\":64, \"##q\":65, \"##S\":66, \"##x\":67, \"##ti\":68, \"##es\":69, \"##on\":70, \"##ng\":71, \"an\":72, \"##er\":73, \"##or\":74, \"##ati\":75, \"and\":76, \"##ers\":77, \"##le\":78, \"##ic\":79, \"##ation\":80, \"##ol\":81, \"##in\":82, \"##it\":83, \"##ri\":84, \"##al\":85, \"##at\":86, \"in\":87, \"##ce\":88, \"##en\":89, \"##ing\":90, \"##ms\":91, \"##ar\":92, \"##ab\":93, \"##ap\":94, \"##pp\":95, \"de\":96, \"##ck\":97, \"##el\":98, ...}))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = wordpiece_tokenizer.encode(\"unbelievable transformation\")\n",
        "print(output.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AxlK5Jqz0r7",
        "outputId": "fa2a0b84-d234-4eed-d774-f017addd4e27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['un', '##b', '##el', '##i', '##e', '##va', '##b', '##le', 'transf', '##orm', '##ation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentencePiece Tokenizer"
      ],
      "metadata": {
        "id": "6k6XCxKGz9uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor"
      ],
      "metadata": {
        "id": "p-Q778bTz2IP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SentencePieceTrainer.Train(\n",
        "    input='corpus.txt', model_prefix='spm_model', vocab_size=100,\n",
        "    model_type='unigram', pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
        "    user_defined_symbols='[MASK]'\n",
        ")"
      ],
      "metadata": {
        "id": "3eNrzkpG0Jix"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = SentencePieceProcessor(model_file='spm_model.model')\n",
        "tokens = sp.encode(\"Sesquipedalophobia\", out_type=str)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbEWrWgM0MnE",
        "outputId": "c99b2dbc-ae60-4b17-e4aa-e90bf8111adb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁S', 'es', 'q', 'u', 'i', 'p', 'e', 'd', 'al', 'o', 'p', 'h', 'o', 'b', 'i', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zR9i3KAe0THR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}