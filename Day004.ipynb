{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58265de8"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to train a simple word-level LSTM model to generate text based on Shakespeare's writing.\n",
        "\n",
        "Here's a breakdown of the key steps:\n",
        "\n",
        "1.  **Data Loading and Preprocessing**: The code loads a text file (`tiny_shakespeare.txt`), tokenizes it into words, creates a vocabulary of unique words, and maps each word to an index. It then prepares the data into sequences of words to be used as input and the next word in the sequence as the target.\n",
        "2.  **Model Definition**: An LSTM (Long Short-Term Memory) neural network model is defined. This model includes an embedding layer to represent words as vectors, an LSTM layer to capture sequential dependencies, and a linear layer to predict the next word.\n",
        "3.  **Training**: The model is trained using the prepared data. The training process involves feeding the input sequences to the model, calculating the difference between the model's predictions and the actual next words (loss), and adjusting the model's parameters to minimize this loss.\n",
        "4.  **Prediction**: After training, the model can be used to predict the next word in a given sequence of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJnqlxGi9-lb"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f6d93ab"
      },
      "source": [
        "### Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxyw5Otx-sgn"
      },
      "outputs": [],
      "source": [
        "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "words = text.split()\n",
        "vocab = sorted(set(words))\n",
        "word2idx = {w: idx for idx, w in enumerate(vocab)}\n",
        "idx2word = {idx: w for w, idx in word2idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "seq_length = 5\n",
        "samples = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ4El6-8-wBG"
      },
      "outputs": [],
      "source": [
        "# Create input-target sequences\n",
        "for i in range(len(words) - seq_length):\n",
        "    # Each sample contains a sequence of seq_length words as input and the next word as target\n",
        "    sample = words[i:i + seq_length + 1]\n",
        "    samples.append(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c812639c"
      },
      "source": [
        "### Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTdAc5Cv-3Ld"
      },
      "outputs": [],
      "source": [
        "# Define a custom Dataset for the text data\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, samples, word2idx):\n",
        "        self.samples = samples\n",
        "        self.word2idx = word2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a sample (input sequence and target word)\n",
        "        sample = self.samples[idx]\n",
        "        # Convert input words to indices and create a tensor\n",
        "        input_seq = torch.LongTensor([self.word2idx[w] for w in sample[:-1]])\n",
        "        # Convert the target word to its index and create a tensor\n",
        "        target = self.word2idx[sample[-1]]\n",
        "        return input_seq, torch.tensor(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fyiV9co-6Io"
      },
      "outputs": [],
      "source": [
        "# Create the Dataset and DataLoader\n",
        "dataset = TextDataset(samples, word2idx)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True) # Use DataLoader for batching and shuffling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d5de563"
      },
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkbM1Z3C-6VJ"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM neural network model\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Embedding layer to convert word indices to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # LSTM layer to capture sequential dependencies\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        # Linear layer to predict the next word (output size is vocabulary size)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through embedding layer\n",
        "        x = self.embedding(x)\n",
        "        # Pass embedded input through LSTM layer\n",
        "        # We only need the hidden state from the last time step for prediction\n",
        "        _, (h_n, _) = self.lstm(x)\n",
        "        # Pass the final hidden state through the linear layer\n",
        "        out = self.fc(h_n.squeeze(0))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63f5c021"
      },
      "source": [
        "### Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EFLHKNr-9ym"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = LSTMNet(vocab_size, embed_dim=50, hidden_dim=100) # Create an instance of the LSTM model\n",
        "loss_fn = nn.CrossEntropyLoss() # Define the loss function (Cross-Entropy for classification)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Define the optimizer (Adam) and learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5b8274"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8W0juqz_AOB",
        "outputId": "30af8257-6a9c-47d4-a6c9-e7a944b0d84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 7.5303\n",
            "Epoch 2, Loss: 6.9131\n",
            "Epoch 3, Loss: 6.6516\n",
            "Epoch 4, Loss: 5.4789\n",
            "Epoch 5, Loss: 5.7635\n",
            "Epoch 6, Loss: 6.4038\n",
            "Epoch 7, Loss: 5.8948\n",
            "Epoch 8, Loss: 5.9343\n",
            "Epoch 9, Loss: 4.2164\n",
            "Epoch 10, Loss: 5.1814\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 20 # Define the number of training epochs\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "        loss.backward() # Compute gradients\n",
        "        optimizer.step() # Update model parameters\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35b799d4"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vgTNi7E0_Cxk"
      },
      "outputs": [],
      "source": [
        "# Function to predict the next word\n",
        "def predict_next_word(model, text_seq, word2idx, idx2word, seq_length):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    # Tokenize the input text sequence and take the last 'seq_length' words\n",
        "    tokens = text_seq.lower().split()[-seq_length:]\n",
        "    # Convert tokens to indices, handle unknown words with index 0, and add batch dimension\n",
        "    input_seq = torch.LongTensor([word2idx.get(w, 0) for w in tokens]).unsqueeze(0)\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        # Get model output (predictions for the next word)\n",
        "        output = model(input_seq)\n",
        "        # Get the index of the predicted next word (highest probability)\n",
        "        pred_idx = torch.argmax(output, dim=1).item()\n",
        "    # Convert the predicted index back to a word\n",
        "    return idx2word[pred_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9EY8pcZ9_FbX",
        "outputId": "a66732ff-dd3d-4d7b-830c-82d71bb62878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Next word prediction: to\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the prediction function\n",
        "input_sentence = \"shall i compare thee\"\n",
        "# Predict the next word based on the input sentence\n",
        "next_word = predict_next_word(model, input_sentence, word2idx, idx2word, seq_length)\n",
        "print(\"Next word prediction:\", next_word)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}