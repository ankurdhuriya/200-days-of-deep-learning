{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5bdf669"
      },
      "source": [
        "## Notebook Overview\n",
        "\n",
        "This notebook explores three popular transformer models from the Hugging Face library: GPT-2, BERT, and T5. We demonstrate basic usage of each model for different natural language processing tasks:\n",
        "\n",
        "-   **GPT-2 (Generative Pre-trained Transformer 2):** Used for text generation, predicting the next word in a sequence.\n",
        "-   **BERT (Bidirectional Encoder Representations from Transformers):** Used for masked language modeling, predicting a masked word in a sentence based on the surrounding context.\n",
        "-   **T5 (Text-to-Text Transfer Transformer):** Used for sequence-to-sequence tasks, demonstrating its ability to process an input sequence and generate an output sequence (in this case, a simplified summarization example).\n",
        "\n",
        "## Simple Differences in Working\n",
        "\n",
        "While all three models are based on the transformer architecture, they are pre-trained on different tasks and designed for different purposes:\n",
        "\n",
        "-   **GPT-2:** Primarily a **decoder-only** model pre-trained on a language modeling objective (predicting the next token). It's excellent for generating coherent and contextually relevant text.\n",
        "-   **BERT:** A **encoder-only** model pre-trained on masked language modeling and next sentence prediction tasks. It's best suited for understanding the context and meaning of text, making it ideal for tasks like classification, named entity recognition, and answering questions about text.\n",
        "-   **T5:** An **encoder-decoder** model pre-trained on a text-to-text framework, where every NLP task is formulated as a text-to-text problem. This versatility allows it to handle a wide range of tasks, including summarization, translation, question answering, and more, by simply changing the input prompt.\n",
        "\n",
        "In essence:\n",
        "- **GPT-2 generates text.**\n",
        "- **BERT understands text (by filling in blanks).**\n",
        "- **T5 transforms text from one form to another.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pyIC7tDTGkPw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdkmaRyoHSoW",
        "outputId": "6584541c-be1d-4915-a0eb-87f11d26014d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"India is the fastest growing\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ft7rrkTH_Ad",
        "outputId": "c124867d-1904-4242-f689-7fe6a9484052"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y160pZFNKBUr",
        "outputId": "aba75fb2-d7ef-4186-ff08-2ef7f9ba38d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[21569,   318,   262, 14162,  3957]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs['input_ids'].tolist()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x9RtE_e5KUsb",
        "outputId": "4739db36-c75a-4fd3-e08d-1e4b695733e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'India is the fastest growing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewcsf2QFIrQK",
        "outputId": "87afb235-faaf-41c1-a297-0c17b121c4f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.7621, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLUZBB4uIzQC",
        "outputId": "5e9b8cd5-dd07-4041-d725-8611c2f35b0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eb659be",
        "outputId": "da3c0323-2a10-482d-9f00-a601071f2b42"
      },
      "source": [
        "last_token_logits = logits[0, -1, :]\n",
        "\n",
        "predicted_token_id = torch.argmax(last_token_logits)\n",
        "\n",
        "predicted_word = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"The predicted next word is: {predicted_word}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted next word is:  country\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM"
      ],
      "metadata": {
        "id": "L_wtzcB3MxZ5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQR_YN7eM4eo",
        "outputId": "5fb5411b-4a08-4267-886e-cc677bdfbac1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Masked language [MASK] is used for pretraining.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "id": "jDUMUspMMzMT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSag7fVrM55G",
        "outputId": "d4359fc0-b8bd-41fd-8609-bc7ef1103d6f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.9433, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z9PeuWSNFed",
        "outputId": "c489ddcf-a84e-48e1-ce2b-d0824ae774a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101, 16520,  2653,   103,  2003,  2109,  2005,  3653, 23654,  2075,\n",
              "          1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdZjgG6WND6S",
        "outputId": "d6d9a7e1-1629-4c09-e9c1-f10de9304247"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ -6.7650,  -6.7192,  -6.6993,  ...,  -6.0738,  -5.8606,  -4.3139],\n",
              "         [ -7.8068,  -8.2320,  -8.0335,  ...,  -6.9457,  -6.9095,  -6.3418],\n",
              "         [ -9.7934,  -9.8951,  -9.7592,  ...,  -8.0059,  -8.1619,  -8.2921],\n",
              "         ...,\n",
              "         [-12.3430, -12.7044, -12.4538,  ...,  -9.8909,  -9.1166,  -8.7265],\n",
              "         [-11.3576, -10.9756, -11.5494,  ...,  -9.3977, -10.7534,  -6.8493],\n",
              "         [-11.9643, -11.5468, -11.7930,  ...,  -8.9498,  -9.5833,  -8.1403]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.mask_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWtRg_13NEb3",
        "outputId": "b6ba1726-ca47-4064-e596-b767e4c3f6b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_index = inputs['input_ids'][0].tolist().index(tokenizer.mask_token_id)"
      ],
      "metadata": {
        "id": "OhkNlf1JNX_r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjPdnl2LNi3r",
        "outputId": "a9c6112d-fb38-4654-e661-d62bae17bc58"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 12, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_token_logits = logits[0, masked_index, :]"
      ],
      "metadata": {
        "id": "0x_uzr_WNfxu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_token_id = torch.argmax(masked_token_logits)"
      ],
      "metadata": {
        "id": "9xapdj3vNiMu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_word = tokenizer.decode(predicted_token_id)"
      ],
      "metadata": {
        "id": "ZYPWCWJbNmeX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The predicted word for the [MASK] token is: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swyk08EkNoik",
        "outputId": "477aeed7-7b7b-4156-aa12-17f1017c6b6e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted word for the [MASK] token is: training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "quaO6XKPN136"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56RpfadBN3JK",
        "outputId": "32d91ede-4c01-4c75-ad4d-bf209792ccee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"summarize: PyTorch enables easy model development.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "id": "RRExL1XwN4PF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDcJ7tEvN7Te",
        "outputId": "41ea338d-5bf0-4373-f0fd-22800e094ca1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[21603,    10, 12901,   382,   127,   524,     3,  7161,   514,   825,\n",
              "           606,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w2wOvroN716",
        "outputId": "edaa917b-82f8-41e7-c9c2-bb009cdd907d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7242, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa2jBPUOC5I",
        "outputId": "0d8b7efb-3513-4fb9-d4a8-6944d27d611e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 13, 32128])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ea5864",
        "outputId": "de770a5c-b1b6-44ee-8309-3d8675e3d422"
      },
      "source": [
        "predicted_token_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "decoded_output = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"The decoded output is: {decoded_output}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decoded output is: Py  Torch enables easy model development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5rV3KxYWOdBj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyPchPi0PpPn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}