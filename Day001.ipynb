{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron Overview and Training Explanation\n",
        "\n",
        "The Perceptron is one of the simplest types of artificial neural networks and serves as a fundamental building block for more complex models. It is designed to perform binary classification by learning a linear decision boundary that separates two classes.\n",
        "\n",
        "### What the Code Does:\n",
        "\n",
        "1. **Initialization**  \n",
        "   We start by randomly initializing the perceptronâ€™s weights and bias. Each weight corresponds to an input feature, and the bias acts as an offset that helps the decision boundary shift to better separate the data.\n",
        "\n",
        "2. **Prediction**  \n",
        "   For each input instance, the perceptron calculates a weighted sum of the input features plus the bias. This sum is then passed through a simple step function that outputs either 0 or 1, corresponding to two classes.\n",
        "\n",
        "3. **Training (Learning)**  \n",
        "   During training, the perceptron iterates over the dataset multiple times (epochs). For each example:\n",
        "   - It predicts the class using current weights and bias.\n",
        "   - Computes the error as the difference between the true label and the predicted label.\n",
        "   - Updates the weights and bias to reduce the error, nudging the decision boundary closer to correctly classify that example.\n",
        "   \n",
        "   The weight update is proportional to the error, the learning rate (which controls the size of the adjustment), and the input feature value. The bias update is proportional to the error and learning rate (considering the bias as a weight connected to a fixed input of 1).\n",
        "\n",
        "4. **Convergence Check**  \n",
        "   After each epoch, the code checks if there were any misclassifications. If the perceptron correctly classifies all examples (total error is zero), training stops early since the model has converged on a solution.\n",
        "\n",
        "### Important Points\n",
        "\n",
        "- **Linearity**: The perceptron can only solve problems where the two classes are linearly separable, meaning a straight line (or hyperplane) can separate the classes perfectly.\n",
        "  \n",
        "- **Failing on Non-linear Problems**: For problems like XOR, which cannot be separated by a single line, the perceptron will fail to converge.\n",
        "\n",
        "- **Learning Rate & Epochs**: The learning rate governs how big each weight update step is, while epochs determine the maximum number of passes over the training data.\n",
        "\n",
        "This simple yet powerful algorithm forms the basis for understanding neural networks and demonstrates fundamental concepts of training via error correction and weight adjustment."
      ],
      "metadata": {
        "id": "yw28aEQ1XLoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AWpOJxFOSeax"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    \"\"\"\n",
        "    A simple implementation of a single-layer perceptron for binary classification.\n",
        "\n",
        "    The perceptron learns a linear decision boundary by iteratively adjusting weights\n",
        "    and bias based on the training data. It uses a step activation function to output\n",
        "    either 0 or 1.\n",
        "\n",
        "    Attributes:\n",
        "        weights (np.ndarray): The weight vector corresponding to each input feature.\n",
        "        bias (float): The bias term added to the weighted sum.\n",
        "        learning_rate (float): Step size for weight updates.\n",
        "        epochs (int): Maximum number of iterations over the training dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_inputs, learning_rate=0.1, epochs=100):\n",
        "        \"\"\"\n",
        "        Initialize the perceptron with random weights and bias.\n",
        "\n",
        "        Args:\n",
        "            num_inputs (int): Number of input features.\n",
        "            learning_rate (float): Learning rate for the weight updates.\n",
        "            epochs (int): Maximum number of training iterations.\n",
        "        \"\"\"\n",
        "        self.weights = np.random.rand(num_inputs)  # Initialize weights randomly\n",
        "        self.bias = np.random.rand(1)              # Initialize bias randomly\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def _step_function(self, x):\n",
        "        \"\"\"\n",
        "        Step activation function that maps input to binary output.\n",
        "\n",
        "        Args:\n",
        "            x (float): The input value, typically the weighted sum of inputs plus bias.\n",
        "\n",
        "        Returns:\n",
        "            int: 1 if x is greater than or equal to 0, else 0.\n",
        "        \"\"\"\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"\n",
        "        Predict the binary class label for given input features.\n",
        "\n",
        "        Calculates the weighted sum of inputs and bias, then applies the step function.\n",
        "\n",
        "        Args:\n",
        "            inputs (np.ndarray): Input features as a 1D array.\n",
        "\n",
        "        Returns:\n",
        "            int: Predicted class label (0 or 1).\n",
        "        \"\"\"\n",
        "        # Calculate weighted sum of inputs and add bias\n",
        "        linear_output = np.dot(inputs, self.weights) + self.bias\n",
        "        # Apply step function to get binary prediction\n",
        "        return self._step_function(linear_output)\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "        \"\"\"\n",
        "        Train the perceptron on the provided dataset.\n",
        "\n",
        "        Iteratively updates weights and bias based on prediction errors to reduce misclassifications.\n",
        "        Stops early if the perceptron converges (no prediction errors).\n",
        "\n",
        "        Args:\n",
        "            training_inputs (np.ndarray): 2D array where each row is an input sample.\n",
        "            labels (np.ndarray): 1D array of true binary class labels corresponding to inputs.\n",
        "        \"\"\"\n",
        "        print(f\"Weight at step 0 : {self.weights}\")\n",
        "        for epoch in range(self.epochs):\n",
        "            total_error = 0\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                prediction = self.predict(inputs)\n",
        "                error = label - prediction\n",
        "\n",
        "                # Update weights: proportional to error, learning rate, and input magnitude\n",
        "                self.weights += self.learning_rate * error * inputs\n",
        "\n",
        "                print(f\"Weight at step {epoch+1} : {self.weights}\")\n",
        "\n",
        "                # Update bias: proportional to error and learning rate (bias input assumed 1)\n",
        "                self.bias += self.learning_rate * error\n",
        "\n",
        "                # Accumulate total error magnitude for this epoch\n",
        "                total_error += abs(error)\n",
        "\n",
        "            # If no errors during this epoch, training has converged\n",
        "            if total_error == 0:\n",
        "                print(f\"Converged after {epoch + 1} epochs.\")\n",
        "                break\n",
        "        else:\n",
        "            # Reached maximum epochs without perfect convergence\n",
        "            print(f\"Training finished after {self.epochs} epochs.\")\n"
      ],
      "metadata": {
        "id": "srt74vigUvC-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AND Gate ---\n",
        "print(\"--- AND Gate ---\")\n",
        "and_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "and_labels = np.array([0, 0, 0, 1])\n",
        "\n",
        "perceptron_and = Perceptron(num_inputs=2)\n",
        "perceptron_and.train(and_inputs, and_labels)\n",
        "\n",
        "for inputs, label in zip(and_inputs, and_labels):\n",
        "    prediction = perceptron_and.predict(inputs)\n",
        "    print(f\"AND({inputs[0]}, {inputs[1]}) -> Predicted: {prediction}, Actual: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nazbspoVgWO",
        "outputId": "8612ee6d-9d3c-4100-9fc2-6d40e2f141de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- AND Gate ---\n",
            "Weight at step 0 : [0.52842781 0.80654865]\n",
            "Weight at step 1 : [0.52842781 0.80654865]\n",
            "Weight at step 1 : [0.52842781 0.70654865]\n",
            "Weight at step 1 : [0.42842781 0.70654865]\n",
            "Weight at step 1 : [0.42842781 0.70654865]\n",
            "Weight at step 2 : [0.42842781 0.70654865]\n",
            "Weight at step 2 : [0.42842781 0.60654865]\n",
            "Weight at step 2 : [0.32842781 0.60654865]\n",
            "Weight at step 2 : [0.32842781 0.60654865]\n",
            "Weight at step 3 : [0.32842781 0.60654865]\n",
            "Weight at step 3 : [0.32842781 0.50654865]\n",
            "Weight at step 3 : [0.22842781 0.50654865]\n",
            "Weight at step 3 : [0.22842781 0.50654865]\n",
            "Weight at step 4 : [0.22842781 0.50654865]\n",
            "Weight at step 4 : [0.22842781 0.40654865]\n",
            "Weight at step 4 : [0.22842781 0.40654865]\n",
            "Weight at step 4 : [0.22842781 0.40654865]\n",
            "Weight at step 5 : [0.22842781 0.40654865]\n",
            "Weight at step 5 : [0.22842781 0.30654865]\n",
            "Weight at step 5 : [0.22842781 0.30654865]\n",
            "Weight at step 5 : [0.22842781 0.30654865]\n",
            "Weight at step 6 : [0.22842781 0.30654865]\n",
            "Weight at step 6 : [0.22842781 0.30654865]\n",
            "Weight at step 6 : [0.22842781 0.30654865]\n",
            "Weight at step 6 : [0.22842781 0.30654865]\n",
            "Converged after 6 epochs.\n",
            "AND(0, 0) -> Predicted: 0, Actual: 0\n",
            "AND(0, 1) -> Predicted: 0, Actual: 0\n",
            "AND(1, 0) -> Predicted: 0, Actual: 0\n",
            "AND(1, 1) -> Predicted: 1, Actual: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- OR Gate ---\n",
        "print(\"\\n--- OR Gate ---\")\n",
        "or_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "or_labels = np.array([0, 1, 1, 1])\n",
        "\n",
        "perceptron_or = Perceptron(num_inputs=2)\n",
        "perceptron_or.train(or_inputs, or_labels)\n",
        "\n",
        "for inputs, label in zip(or_inputs, or_labels):\n",
        "    prediction = perceptron_or.predict(inputs)\n",
        "    print(f\"OR({inputs[0]}, {inputs[1]}) -> Predicted: {prediction}, Actual: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnk2HHtwViUV",
        "outputId": "e7027619-f0f7-4f79-c39b-8bff0ba01918"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- OR Gate ---\n",
            "Weight at step 0 : [0.58507941 0.2360992 ]\n",
            "Weight at step 1 : [0.58507941 0.2360992 ]\n",
            "Weight at step 1 : [0.58507941 0.2360992 ]\n",
            "Weight at step 1 : [0.58507941 0.2360992 ]\n",
            "Weight at step 1 : [0.58507941 0.2360992 ]\n",
            "Weight at step 2 : [0.58507941 0.2360992 ]\n",
            "Weight at step 2 : [0.58507941 0.2360992 ]\n",
            "Weight at step 2 : [0.58507941 0.2360992 ]\n",
            "Weight at step 2 : [0.58507941 0.2360992 ]\n",
            "Weight at step 3 : [0.58507941 0.2360992 ]\n",
            "Weight at step 3 : [0.58507941 0.2360992 ]\n",
            "Weight at step 3 : [0.58507941 0.2360992 ]\n",
            "Weight at step 3 : [0.58507941 0.2360992 ]\n",
            "Converged after 3 epochs.\n",
            "OR(0, 0) -> Predicted: 0, Actual: 0\n",
            "OR(0, 1) -> Predicted: 1, Actual: 1\n",
            "OR(1, 0) -> Predicted: 1, Actual: 1\n",
            "OR(1, 1) -> Predicted: 1, Actual: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- XOR Gate (Expected to Fail) ---\n",
        "print(\"\\n--- XOR Gate (Expected to Fail) ---\")\n",
        "xor_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "xor_labels = np.array([0, 1, 1, 0])\n",
        "\n",
        "perceptron_xor = Perceptron(num_inputs=2, epochs=50) # Increase epochs to show it doesn't converge\n",
        "perceptron_xor.train(xor_inputs, xor_labels)\n",
        "\n",
        "for inputs, label in zip(xor_inputs, xor_labels):\n",
        "    prediction = perceptron_xor.predict(inputs)\n",
        "    print(f\"XOR({inputs[0]}, {inputs[1]}) -> Predicted: {prediction}, Actual: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "71iRo4r2VkzM",
        "outputId": "7bf87d4d-d7e8-4752-a012-a8c67aa522f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- XOR Gate (Expected to Fail) ---\n",
            "Weight at step 0 : [0.1188553  0.46465484]\n",
            "Weight at step 1 : [0.1188553  0.46465484]\n",
            "Weight at step 1 : [0.1188553  0.46465484]\n",
            "Weight at step 1 : [0.1188553  0.46465484]\n",
            "Weight at step 1 : [0.0188553  0.36465484]\n",
            "Weight at step 2 : [0.0188553  0.36465484]\n",
            "Weight at step 2 : [0.0188553  0.36465484]\n",
            "Weight at step 2 : [0.0188553  0.36465484]\n",
            "Weight at step 2 : [-0.0811447   0.26465484]\n",
            "Weight at step 3 : [-0.0811447   0.26465484]\n",
            "Weight at step 3 : [-0.0811447   0.26465484]\n",
            "Weight at step 3 : [0.0188553  0.26465484]\n",
            "Weight at step 3 : [-0.0811447   0.16465484]\n",
            "Weight at step 4 : [-0.0811447   0.16465484]\n",
            "Weight at step 4 : [-0.0811447   0.16465484]\n",
            "Weight at step 4 : [0.0188553  0.16465484]\n",
            "Weight at step 4 : [-0.0811447   0.06465484]\n",
            "Weight at step 5 : [-0.0811447   0.06465484]\n",
            "Weight at step 5 : [-0.0811447   0.16465484]\n",
            "Weight at step 5 : [0.0188553  0.16465484]\n",
            "Weight at step 5 : [-0.0811447   0.06465484]\n",
            "Weight at step 6 : [-0.0811447   0.06465484]\n",
            "Weight at step 6 : [-0.0811447   0.16465484]\n",
            "Weight at step 6 : [0.0188553  0.16465484]\n",
            "Weight at step 6 : [-0.0811447   0.06465484]\n",
            "Weight at step 7 : [-0.0811447   0.06465484]\n",
            "Weight at step 7 : [-0.0811447   0.16465484]\n",
            "Weight at step 7 : [0.0188553  0.16465484]\n",
            "Weight at step 7 : [-0.0811447   0.06465484]\n",
            "Weight at step 8 : [-0.0811447   0.06465484]\n",
            "Weight at step 8 : [-0.0811447   0.16465484]\n",
            "Weight at step 8 : [0.0188553  0.16465484]\n",
            "Weight at step 8 : [-0.0811447   0.06465484]\n",
            "Weight at step 9 : [-0.0811447   0.06465484]\n",
            "Weight at step 9 : [-0.0811447   0.16465484]\n",
            "Weight at step 9 : [0.0188553  0.16465484]\n",
            "Weight at step 9 : [-0.0811447   0.06465484]\n",
            "Weight at step 10 : [-0.0811447   0.06465484]\n",
            "Weight at step 10 : [-0.0811447   0.16465484]\n",
            "Weight at step 10 : [0.0188553  0.16465484]\n",
            "Weight at step 10 : [-0.0811447   0.06465484]\n",
            "Weight at step 11 : [-0.0811447   0.06465484]\n",
            "Weight at step 11 : [-0.0811447   0.16465484]\n",
            "Weight at step 11 : [0.0188553  0.16465484]\n",
            "Weight at step 11 : [-0.0811447   0.06465484]\n",
            "Weight at step 12 : [-0.0811447   0.06465484]\n",
            "Weight at step 12 : [-0.0811447   0.16465484]\n",
            "Weight at step 12 : [0.0188553  0.16465484]\n",
            "Weight at step 12 : [-0.0811447   0.06465484]\n",
            "Weight at step 13 : [-0.0811447   0.06465484]\n",
            "Weight at step 13 : [-0.0811447   0.16465484]\n",
            "Weight at step 13 : [0.0188553  0.16465484]\n",
            "Weight at step 13 : [-0.0811447   0.06465484]\n",
            "Weight at step 14 : [-0.0811447   0.06465484]\n",
            "Weight at step 14 : [-0.0811447   0.16465484]\n",
            "Weight at step 14 : [0.0188553  0.16465484]\n",
            "Weight at step 14 : [-0.0811447   0.06465484]\n",
            "Weight at step 15 : [-0.0811447   0.06465484]\n",
            "Weight at step 15 : [-0.0811447   0.16465484]\n",
            "Weight at step 15 : [0.0188553  0.16465484]\n",
            "Weight at step 15 : [-0.0811447   0.06465484]\n",
            "Weight at step 16 : [-0.0811447   0.06465484]\n",
            "Weight at step 16 : [-0.0811447   0.16465484]\n",
            "Weight at step 16 : [0.0188553  0.16465484]\n",
            "Weight at step 16 : [-0.0811447   0.06465484]\n",
            "Weight at step 17 : [-0.0811447   0.06465484]\n",
            "Weight at step 17 : [-0.0811447   0.16465484]\n",
            "Weight at step 17 : [0.0188553  0.16465484]\n",
            "Weight at step 17 : [-0.0811447   0.06465484]\n",
            "Weight at step 18 : [-0.0811447   0.06465484]\n",
            "Weight at step 18 : [-0.0811447   0.16465484]\n",
            "Weight at step 18 : [0.0188553  0.16465484]\n",
            "Weight at step 18 : [-0.0811447   0.06465484]\n",
            "Weight at step 19 : [-0.0811447   0.06465484]\n",
            "Weight at step 19 : [-0.0811447   0.16465484]\n",
            "Weight at step 19 : [0.0188553  0.16465484]\n",
            "Weight at step 19 : [-0.0811447   0.06465484]\n",
            "Weight at step 20 : [-0.0811447   0.06465484]\n",
            "Weight at step 20 : [-0.0811447   0.16465484]\n",
            "Weight at step 20 : [0.0188553  0.16465484]\n",
            "Weight at step 20 : [-0.0811447   0.06465484]\n",
            "Weight at step 21 : [-0.0811447   0.06465484]\n",
            "Weight at step 21 : [-0.0811447   0.16465484]\n",
            "Weight at step 21 : [0.0188553  0.16465484]\n",
            "Weight at step 21 : [-0.0811447   0.06465484]\n",
            "Weight at step 22 : [-0.0811447   0.06465484]\n",
            "Weight at step 22 : [-0.0811447   0.16465484]\n",
            "Weight at step 22 : [0.0188553  0.16465484]\n",
            "Weight at step 22 : [-0.0811447   0.06465484]\n",
            "Weight at step 23 : [-0.0811447   0.06465484]\n",
            "Weight at step 23 : [-0.0811447   0.16465484]\n",
            "Weight at step 23 : [0.0188553  0.16465484]\n",
            "Weight at step 23 : [-0.0811447   0.06465484]\n",
            "Weight at step 24 : [-0.0811447   0.06465484]\n",
            "Weight at step 24 : [-0.0811447   0.16465484]\n",
            "Weight at step 24 : [0.0188553  0.16465484]\n",
            "Weight at step 24 : [-0.0811447   0.06465484]\n",
            "Weight at step 25 : [-0.0811447   0.06465484]\n",
            "Weight at step 25 : [-0.0811447   0.16465484]\n",
            "Weight at step 25 : [0.0188553  0.16465484]\n",
            "Weight at step 25 : [-0.0811447   0.06465484]\n",
            "Weight at step 26 : [-0.0811447   0.06465484]\n",
            "Weight at step 26 : [-0.0811447   0.16465484]\n",
            "Weight at step 26 : [0.0188553  0.16465484]\n",
            "Weight at step 26 : [-0.0811447   0.06465484]\n",
            "Weight at step 27 : [-0.0811447   0.06465484]\n",
            "Weight at step 27 : [-0.0811447   0.16465484]\n",
            "Weight at step 27 : [0.0188553  0.16465484]\n",
            "Weight at step 27 : [-0.0811447   0.06465484]\n",
            "Weight at step 28 : [-0.0811447   0.06465484]\n",
            "Weight at step 28 : [-0.0811447   0.16465484]\n",
            "Weight at step 28 : [0.0188553  0.16465484]\n",
            "Weight at step 28 : [-0.0811447   0.06465484]\n",
            "Weight at step 29 : [-0.0811447   0.06465484]\n",
            "Weight at step 29 : [-0.0811447   0.16465484]\n",
            "Weight at step 29 : [0.0188553  0.16465484]\n",
            "Weight at step 29 : [-0.0811447   0.06465484]\n",
            "Weight at step 30 : [-0.0811447   0.06465484]\n",
            "Weight at step 30 : [-0.0811447   0.16465484]\n",
            "Weight at step 30 : [0.0188553  0.16465484]\n",
            "Weight at step 30 : [-0.0811447   0.06465484]\n",
            "Weight at step 31 : [-0.0811447   0.06465484]\n",
            "Weight at step 31 : [-0.0811447   0.16465484]\n",
            "Weight at step 31 : [0.0188553  0.16465484]\n",
            "Weight at step 31 : [-0.0811447   0.06465484]\n",
            "Weight at step 32 : [-0.0811447   0.06465484]\n",
            "Weight at step 32 : [-0.0811447   0.16465484]\n",
            "Weight at step 32 : [0.0188553  0.16465484]\n",
            "Weight at step 32 : [-0.0811447   0.06465484]\n",
            "Weight at step 33 : [-0.0811447   0.06465484]\n",
            "Weight at step 33 : [-0.0811447   0.16465484]\n",
            "Weight at step 33 : [0.0188553  0.16465484]\n",
            "Weight at step 33 : [-0.0811447   0.06465484]\n",
            "Weight at step 34 : [-0.0811447   0.06465484]\n",
            "Weight at step 34 : [-0.0811447   0.16465484]\n",
            "Weight at step 34 : [0.0188553  0.16465484]\n",
            "Weight at step 34 : [-0.0811447   0.06465484]\n",
            "Weight at step 35 : [-0.0811447   0.06465484]\n",
            "Weight at step 35 : [-0.0811447   0.16465484]\n",
            "Weight at step 35 : [0.0188553  0.16465484]\n",
            "Weight at step 35 : [-0.0811447   0.06465484]\n",
            "Weight at step 36 : [-0.0811447   0.06465484]\n",
            "Weight at step 36 : [-0.0811447   0.16465484]\n",
            "Weight at step 36 : [0.0188553  0.16465484]\n",
            "Weight at step 36 : [-0.0811447   0.06465484]\n",
            "Weight at step 37 : [-0.0811447   0.06465484]\n",
            "Weight at step 37 : [-0.0811447   0.16465484]\n",
            "Weight at step 37 : [0.0188553  0.16465484]\n",
            "Weight at step 37 : [-0.0811447   0.06465484]\n",
            "Weight at step 38 : [-0.0811447   0.06465484]\n",
            "Weight at step 38 : [-0.0811447   0.16465484]\n",
            "Weight at step 38 : [0.0188553  0.16465484]\n",
            "Weight at step 38 : [-0.0811447   0.06465484]\n",
            "Weight at step 39 : [-0.0811447   0.06465484]\n",
            "Weight at step 39 : [-0.0811447   0.16465484]\n",
            "Weight at step 39 : [0.0188553  0.16465484]\n",
            "Weight at step 39 : [-0.0811447   0.06465484]\n",
            "Weight at step 40 : [-0.0811447   0.06465484]\n",
            "Weight at step 40 : [-0.0811447   0.16465484]\n",
            "Weight at step 40 : [0.0188553  0.16465484]\n",
            "Weight at step 40 : [-0.0811447   0.06465484]\n",
            "Weight at step 41 : [-0.0811447   0.06465484]\n",
            "Weight at step 41 : [-0.0811447   0.16465484]\n",
            "Weight at step 41 : [0.0188553  0.16465484]\n",
            "Weight at step 41 : [-0.0811447   0.06465484]\n",
            "Weight at step 42 : [-0.0811447   0.06465484]\n",
            "Weight at step 42 : [-0.0811447   0.16465484]\n",
            "Weight at step 42 : [0.0188553  0.16465484]\n",
            "Weight at step 42 : [-0.0811447   0.06465484]\n",
            "Weight at step 43 : [-0.0811447   0.06465484]\n",
            "Weight at step 43 : [-0.0811447   0.16465484]\n",
            "Weight at step 43 : [0.0188553  0.16465484]\n",
            "Weight at step 43 : [-0.0811447   0.06465484]\n",
            "Weight at step 44 : [-0.0811447   0.06465484]\n",
            "Weight at step 44 : [-0.0811447   0.16465484]\n",
            "Weight at step 44 : [0.0188553  0.16465484]\n",
            "Weight at step 44 : [-0.0811447   0.06465484]\n",
            "Weight at step 45 : [-0.0811447   0.06465484]\n",
            "Weight at step 45 : [-0.0811447   0.16465484]\n",
            "Weight at step 45 : [0.0188553  0.16465484]\n",
            "Weight at step 45 : [-0.0811447   0.06465484]\n",
            "Weight at step 46 : [-0.0811447   0.06465484]\n",
            "Weight at step 46 : [-0.0811447   0.16465484]\n",
            "Weight at step 46 : [0.0188553  0.16465484]\n",
            "Weight at step 46 : [-0.0811447   0.06465484]\n",
            "Weight at step 47 : [-0.0811447   0.06465484]\n",
            "Weight at step 47 : [-0.0811447   0.16465484]\n",
            "Weight at step 47 : [0.0188553  0.16465484]\n",
            "Weight at step 47 : [-0.0811447   0.06465484]\n",
            "Weight at step 48 : [-0.0811447   0.06465484]\n",
            "Weight at step 48 : [-0.0811447   0.16465484]\n",
            "Weight at step 48 : [0.0188553  0.16465484]\n",
            "Weight at step 48 : [-0.0811447   0.06465484]\n",
            "Weight at step 49 : [-0.0811447   0.06465484]\n",
            "Weight at step 49 : [-0.0811447   0.16465484]\n",
            "Weight at step 49 : [0.0188553  0.16465484]\n",
            "Weight at step 49 : [-0.0811447   0.06465484]\n",
            "Weight at step 50 : [-0.0811447   0.06465484]\n",
            "Weight at step 50 : [-0.0811447   0.16465484]\n",
            "Weight at step 50 : [0.0188553  0.16465484]\n",
            "Weight at step 50 : [-0.0811447   0.06465484]\n",
            "Training finished after 50 epochs.\n",
            "XOR(0, 0) -> Predicted: 1, Actual: 0\n",
            "XOR(0, 1) -> Predicted: 1, Actual: 1\n",
            "XOR(1, 0) -> Predicted: 0, Actual: 1\n",
            "XOR(1, 1) -> Predicted: 1, Actual: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZ63FmPLYUtb"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}