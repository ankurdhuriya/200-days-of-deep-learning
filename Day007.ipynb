{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97cb57c5"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "The key steps covered are:\n",
        "\n",
        "1.  **Library Imports**: Importing necessary PyTorch and other libraries.\n",
        "2.  **Data Download**: Downloading the Tiny Shakespeare text data.\n",
        "3.  **Data Preprocessing**: Converting text to lowercase, creating a vocabulary, and generating word-to-index and index-to-word mappings.\n",
        "4.  **Sample Creation**: Generating input-target pairs for training, where each input is a sequence of words and the target is the next word.\n",
        "5.  **Dataset and DataLoader**: Defining a custom PyTorch Dataset and creating a DataLoader for efficient batch processing.\n",
        "6.  **Model Definition**:\n",
        "    *   **Encoder**: An LSTM-based encoder to process the input sequence.\n",
        "    *   **Bahdanau Attention**: An implementation of the additive attention mechanism.\n",
        "    *   **Attention Decoder**: An LSTM-based decoder that uses attention to predict the next word.\n",
        "    *   **Seq2Seq**: The main model combining the encoder and decoder with teacher forcing.\n",
        "7.  **Model Initialization**: Initializing the model and moving it to the appropriate device (CPU/GPU).\n",
        "8.  **Training**: Defining the loss function and optimizer, and implementing the training loop with backpropagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb4e8bab"
      },
      "source": [
        "### Library Imports\n",
        "This cell imports the necessary libraries for building and training a recurrent neural network (RNN) with attention for text generation.\n",
        "- `torch`, `torch.nn`, `torch.optim`: Core PyTorch libraries for building neural networks and optimizers.\n",
        "- `torch.utils.data.Dataset`, `DataLoader`: Utilities for handling datasets and creating data loaders for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q7WVOqkG8iY1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eed0936c"
      },
      "source": [
        "### Download Tiny Shakespeare Dataset\n",
        "This cell downloads the \"tiny Shakespeare\" text dataset from a GitHub repository using the `requests` library and saves it as a text file named `tiny_shakespeare.txt`. This dataset is commonly used for character-level or word-level language modeling tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69500cae",
        "outputId": "e89a4a40-cecf-4f1d-f194-57a33a635f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiny_shakespeare.txt created successfully.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "with open(\"tiny_shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "print(\"tiny_shakespeare.txt created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "472f87b7"
      },
      "source": [
        "### Preprocess Text Data\n",
        "This cell preprocesses the downloaded text data:\n",
        "- It reads the text from `tiny_shakespeare.txt` and converts it to lowercase.\n",
        "- It splits the text into a list of words.\n",
        "- It creates a sorted vocabulary of unique words.\n",
        "- It creates mappings between words and their corresponding indices (`word2idx`) and vice versa (`idx2word`).\n",
        "- It defines `seq_length` as the length of the input sequence for the model.\n",
        "- It initializes an empty list `samples` to store the input-target pairs for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "stJWaf0m-ZPK"
      },
      "outputs": [],
      "source": [
        "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "\n",
        "words = text.split()\n",
        "vocab = sorted(set(words))\n",
        "word2idx = {w: idx for idx, w in enumerate(vocab)}\n",
        "idx2word = {idx: w for w, idx in word2idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "seq_length = 5\n",
        "samples = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b242844c"
      },
      "source": [
        "### Create Samples\n",
        "This cell generates training samples from the preprocessed words. Each sample consists of a sequence of `seq_length` words as input and the immediately following word as the target. These samples will be used to train the model to predict the next word in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wBigvm-d-ssz"
      },
      "outputs": [],
      "source": [
        "for i in range(len(words) - seq_length):\n",
        "    # Each sample contains a sequence of seq_length words as input and the next word as target\n",
        "    sample = words[i:i + seq_length + 1]\n",
        "    samples.append(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48df2852"
      },
      "source": [
        "### Text Dataset Class\n",
        "This cell defines a custom PyTorch `Dataset` class named `TextDataset`.\n",
        "- The `__init__` method initializes the dataset with the generated samples and the word-to-index mapping.\n",
        "- The `__len__` method returns the total number of samples in the dataset.\n",
        "- The `__getitem__` method takes an index and returns a single sample as a tuple: the input sequence of word indices (as a `LongTensor`) and the target word index (as a `tensor`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dfhjjg-A-us8"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, samples, word2idx):\n",
        "        self.samples = samples\n",
        "        self.word2idx = word2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a sample (input sequence and target word)\n",
        "        sample = self.samples[idx]\n",
        "        # Convert input words to indices and create a tensor\n",
        "        input_seq = torch.LongTensor([self.word2idx[w] for w in sample[:-1]])\n",
        "        # Convert the target word to its index and create a tensor\n",
        "        target = self.word2idx[sample[-1]]\n",
        "        return input_seq, torch.tensor(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "220bed42"
      },
      "source": [
        "### Create Dataset and DataLoader\n",
        "This cell creates an instance of the `TextDataset` using the generated samples and word-to-index mapping. It then creates a `DataLoader` to efficiently load batches of data during training. The `batch_size` is set to 128, and `shuffle=True` shuffles the data at the beginning of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eIZVFb6E-wxb"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(samples, word2idx)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33a5eb84"
      },
      "source": [
        "### Model Definition (Encoder, Attention, Decoder, Seq2Seq)\n",
        "This cell defines the neural network architecture:\n",
        "- **`Encoder`**: An LSTM-based encoder that processes the input sequence and outputs context vectors (hidden and cell states).\n",
        "- **`BahdanauAttention`**: An implementation of the additive Bahdanau attention mechanism to calculate attention weights.\n",
        "- **`AttentionDecoder`**: An LSTM-based decoder that uses the attention mechanism to focus on relevant parts of the input sequence while generating the output.\n",
        "- **`Seq2Seq`**: The main sequence-to-sequence model that combines the encoder and decoder. It includes a teacher forcing mechanism during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "maAahhvR-ztV"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: [src_len, batch_size]\n",
        "        embedded = self.embedding(src)  # embedded: [src_len, batch_size, emb_dim]\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        # outputs: [src_len, batch_size, hid_dim]\n",
        "        # hidden, cell: [n_layers, batch_size, hid_dim]\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "# Additive Bahdanau Attention module\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
        "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, dec_hidden_dim]\n",
        "        # encoder_outputs: [src_len, batch_size, enc_hidden_dim]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # Repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, dec_hidden_dim]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, enc_hidden_dim]\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch, src_len, dec_hidden_dim]\n",
        "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
        "        return nn.functional.softmax(attention, dim=1)\n",
        "\n",
        "# Modified Decoder with Attention\n",
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.attention = BahdanauAttention(enc_hidden_dim, dec_hidden_dim)\n",
        "        self.lstm = nn.LSTM(enc_hidden_dim + emb_dim, dec_hidden_dim, n_layers)\n",
        "        self.fc_out = nn.Linear(enc_hidden_dim + dec_hidden_dim + emb_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        # input: [batch_size]\n",
        "        # hidden, cell: [n_layers, batch_size, dec_hidden_dim]\n",
        "        # encoder_outputs: [src_len, batch_size, enc_hidden_dim]\n",
        "\n",
        "        input = input.unsqueeze(0)  # [1, batch_size]\n",
        "        embedded = self.embedding(input)  # [1, batch_size, emb_dim]\n",
        "\n",
        "        dec_hidden = hidden[-1]  # get last layer hidden state [batch_size, dec_hidden_dim]\n",
        "\n",
        "        # Compute attention weights and context vector\n",
        "        a = self.attention(dec_hidden, encoder_outputs)  # [batch_size, src_len]\n",
        "        a = a.unsqueeze(1)  # [batch_size, 1, src_len]\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, enc_hidden_dim]\n",
        "\n",
        "        # Weighted sum context vector\n",
        "        context = torch.bmm(a, encoder_outputs)  # [batch_size, 1, enc_hidden_dim]\n",
        "        context = context.permute(1, 0, 2)  # [1, batch_size, enc_hidden_dim]\n",
        "\n",
        "        # LSTM input is concatenation of embedded input and context vector\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)  # [1, batch_size, emb_dim + enc_hidden_dim]\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "\n",
        "        output = output.squeeze(0)   # [batch_size, dec_hidden_dim]\n",
        "        context = context.squeeze(0) # [batch_size, enc_hidden_dim]\n",
        "        embedded = embedded.squeeze(0) # [batch_size, emb_dim]\n",
        "\n",
        "        output = self.fc_out(torch.cat((output, context, embedded), dim=1))  # [batch_size, output_dim]\n",
        "\n",
        "        return output, hidden, cell, a.squeeze(1)  # a is attention weights for visualization\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs) # Pass encoder_outputs to decoder\n",
        "            outputs[t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SqmKk4bg_xoH"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "840d3057"
      },
      "source": [
        "### Initialize Model\n",
        "This cell initializes instances of the `Encoder`, `AttentionDecoder`, and `Seq2Seq` models with specified dimensions. The models are then moved to the selected device (GPU or CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mquzsgeV-4ny"
      },
      "outputs": [],
      "source": [
        "enc = Encoder(vocab_size, emb_dim=64, hid_dim=128).to(device)\n",
        "dec = AttentionDecoder(vocab_size, emb_dim=64, enc_hidden_dim=128, dec_hidden_dim=128).to(device)\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3d47f8"
      },
      "source": [
        "### Define Loss Function, Optimizer, and Training Loop\n",
        "This cell defines the loss function (`CrossEntropyLoss` for classification), the optimizer (`Adam`), and the training loop.\n",
        "- The loop iterates for a specified number of epochs (`num_epochs`).\n",
        "- In each epoch, it iterates through the `dataloader` to get batches of inputs and targets.\n",
        "- It prepares the target sequence for the decoder (including teacher forcing).\n",
        "- It performs the forward pass, calculates the loss, and performs backpropagation and optimization.\n",
        "- It prints the average loss for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7VrrO2O_vBG",
        "outputId": "f5515ffb-c926-47aa-9290-9b44e8c839c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 7.2381\n",
            "Epoch [2/10], Loss: 6.2037\n",
            "Epoch [3/10], Loss: 5.3818\n",
            "Epoch [4/10], Loss: 4.5951\n",
            "Epoch [5/10], Loss: 3.9683\n",
            "Epoch [6/10], Loss: 3.4893\n",
            "Epoch [7/10], Loss: 3.1010\n",
            "Epoch [8/10], Loss: 2.7695\n",
            "Epoch [9/10], Loss: 2.4778\n",
            "Epoch [10/10], Loss: 2.2180\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop for next-word prediction\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.transpose(0, 1).to(device)  # [seq_len, batch]\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Prepare trg sequence for feeding into decoder: input tokens + actual next word to predict\n",
        "        # For next word prediction, trg sequence has length 2: input last word, target word\n",
        "        # So here we'll create trg just for teacher forcing starting with inputs[-1], targets\n",
        "        trg = torch.zeros(2, inputs.shape[1], dtype=torch.long).to(device)\n",
        "        trg[0, :] = inputs[-1, :]\n",
        "        trg[1, :] = targets\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs, trg)\n",
        "        # Output shape: [trg_len, batch, vocab_size], trg_len=2\n",
        "        # Calculate loss only for second token prediction\n",
        "        loss = criterion(output[1], trg[1])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNvw2X6NWPhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}